{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be81a7e9",
   "metadata": {},
   "source": [
    "# Vietnamese Students' Feedback Corpus Download\n",
    "\n",
    "This notebook downloads and saves the Vietnamese Students' Feedback Corpus (UIT-VSFC) data to local files.\n",
    "\n",
    "The dataset consists of over 16,000 sentences which are human-annotated with two different tasks:\n",
    "- Sentiment-based classification (negative, neutral, positive)\n",
    "- Topic-based classification (lecturer, training_program, facility, others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cae4b02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "print(\"Required libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135258d",
   "metadata": {},
   "source": [
    "## Define Dataset URLs\n",
    "\n",
    "The dataset is split into train, validation, and test sets. Each split contains three files:\n",
    "- sentences: The actual feedback text\n",
    "- sentiments: Sentiment labels (0=negative, 1=neutral, 2=positive)\n",
    "- topics: Topic labels (0=lecturer, 1=training_program, 2=facility, 3=others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a579bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URLs defined successfully!\n",
      "Total splits: 3\n",
      "Files per split: 3\n"
     ]
    }
   ],
   "source": [
    "# Define the URLs for downloading the dataset\n",
    "URLS = {\n",
    "    \"train\": {\n",
    "        \"sentences\": \"https://drive.google.com/uc?id=1nzak5OkrheRV1ltOGCXkT671bmjODLhP&export=download\",\n",
    "        \"sentiments\": \"https://drive.google.com/uc?id=1ye-gOZIBqXdKOoi_YxvpT6FeRNmViPPv&export=download\",\n",
    "        \"topics\": \"https://drive.google.com/uc?id=14MuDtwMnNOcr4z_8KdpxprjbwaQ7lJ_C&export=download\",\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"sentences\": \"https://drive.google.com/uc?id=1sMJSR3oRfPc3fe1gK-V3W5F24tov_517&export=download\",\n",
    "        \"sentiments\": \"https://drive.google.com/uc?id=1GiY1AOp41dLXIIkgES4422AuDwmbUseL&export=download\",\n",
    "        \"topics\": \"https://drive.google.com/uc?id=1DwLgDEaFWQe8mOd7EpF-xqMEbDLfdT-W&export=download\",\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"sentences\": \"https://drive.google.com/uc?id=1aNMOeZZbNwSRkjyCWAGtNCMa3YrshR-n&export=download\",\n",
    "        \"sentiments\": \"https://drive.google.com/uc?id=1vkQS5gI0is4ACU58-AbWusnemw7KZNfO&export=download\",\n",
    "        \"topics\": \"https://drive.google.com/uc?id=1_ArMpDguVsbUGl-xSMkTF_p5KpZrmpSB&export=download\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Dataset URLs defined successfully!\")\n",
    "print(f\"Total splits: {len(URLS)}\")\n",
    "print(f\"Files per split: {len(URLS['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e8c5d",
   "metadata": {},
   "source": [
    "## Create Download Directory\n",
    "\n",
    "Create a local directory to store the downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79e88cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download directory created: /Users/ducqhle/Documents/AI_Thinking_DoAn/DoAn/vietnamese_feedback_csv\n"
     ]
    }
   ],
   "source": [
    "# Create download directory\n",
    "download_dir = Path(\"vietnamese_feedback_csv\")\n",
    "download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Download directory created: {download_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ba5fc",
   "metadata": {},
   "source": [
    "## Download Function\n",
    "\n",
    "Define a function to download files from Google Drive URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3645e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def download_file(url, filename, max_retries=3):\n",
    "    \"\"\"Download a file from URL with retry mechanism.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Downloading {filename} (attempt {attempt + 1}/{max_retries})...\")\n",
    "            \n",
    "            # Create a session for better connection handling\n",
    "            session = requests.Session()\n",
    "            \n",
    "            # Set headers to mimic a browser\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            \n",
    "            response = session.get(url, headers=headers, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Save the file\n",
    "            with open(filename, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            \n",
    "            file_size = os.path.getsize(filename)\n",
    "            print(f\"✓ Successfully downloaded {filename} ({file_size} bytes)\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error downloading {filename}: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(f\"Failed to download {filename} after {max_retries} attempts\")\n",
    "                return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "print(\"Download function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e6896",
   "metadata": {},
   "source": [
    "## Download All Files\n",
    "\n",
    "Download all the dataset files for train, validation, and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "319ba11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of 9 files...\n",
      "\n",
      "=== Processing TRAIN split ===\n",
      "Downloading vietnamese_feedback_csv/temp_train_sentences.txt (attempt 1/3)...\n",
      "✓ Successfully downloaded vietnamese_feedback_csv/temp_train_sentences.txt (898090 bytes)\n",
      "Downloading vietnamese_feedback_csv/temp_train_sentiments.txt (attempt 1/3)...\n",
      "✓ Successfully downloaded vietnamese_feedback_csv/temp_train_sentiments.txt (22852 bytes)\n",
      "Downloading vietnamese_feedback_csv/temp_train_topics.txt (attempt 1/3)...\n",
      "✓ Successfully downloaded vietnamese_feedback_csv/temp_train_topics.txt (22852 bytes)\n",
      "Creating CSV file for train split...\n",
      "✓ Created train_data.csv with 11426 records\n",
      "\n",
      "=== Processing VALIDATION split ===\n",
      "Downloading vietnamese_feedback_csv/temp_validation_sentences.txt (attempt 1/3)...\n",
      "✓ Successfully downloaded vietnamese_feedback_csv/temp_validation_sentences.txt (118628 bytes)\n",
      "Downloading vietnamese_feedback_csv/temp_validation_sentiments.txt (attempt 1/3)...\n",
      "✓ Successfully downloaded vietnamese_feedback_csv/temp_validation_sentiments.txt (3166 bytes)\n",
      "Downloading vietnamese_feedback_csv/temp_validation_topics.txt (attempt 1/3)...\n",
      "✓ Successfully downloaded vietnamese_feedback_csv/temp_validation_topics.txt (3166 bytes)\n",
      "Creating CSV file for validation split...\n",
      "✓ Created validation_data.csv with 1583 records\n",
      "\n",
      "=== Processing TEST split ===\n",
      "Downloading vietnamese_feedback_csv/temp_test_sentences.txt (attempt 1/3)...\n",
      "✓ Successfully downloaded vietnamese_feedback_csv/temp_test_sentences.txt (247849 bytes)\n",
      "Downloading vietnamese_feedback_csv/temp_test_sentiments.txt (attempt 1/3)...\n",
      "✓ Successfully downloaded vietnamese_feedback_csv/temp_test_sentiments.txt (6332 bytes)\n",
      "Downloading vietnamese_feedback_csv/temp_test_topics.txt (attempt 1/3)...\n",
      "✓ Successfully downloaded vietnamese_feedback_csv/temp_test_topics.txt (6332 bytes)\n",
      "Creating CSV file for test split...\n",
      "✓ Created test_data.csv with 3166 records\n",
      "\n",
      "Download completed: 9/9 files processed\n"
     ]
    }
   ],
   "source": [
    "# Download all files and create CSV datasets\n",
    "download_results = {}\n",
    "total_files = sum(len(files) for files in URLS.values())\n",
    "downloaded_count = 0\n",
    "\n",
    "print(f\"Starting download of {total_files} files...\\n\")\n",
    "\n",
    "# Define label mappings\n",
    "sentiment_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "topic_labels = [\"lecturer\", \"training_program\", \"facility\", \"others\"]\n",
    "\n",
    "for split_name, files in URLS.items():\n",
    "    print(f\"=== Processing {split_name.upper()} split ===\")\n",
    "    download_results[split_name] = {}\n",
    "    \n",
    "    # Check if CSV already exists\n",
    "    csv_filename = download_dir / f\"{split_name}_data.csv\"\n",
    "    if csv_filename.exists():\n",
    "        print(f\"✓ {csv_filename.name} already exists\")\n",
    "        downloaded_count += len(files)\n",
    "        continue\n",
    "    \n",
    "    # Download temporary files\n",
    "    temp_files = {}\n",
    "    all_success = True\n",
    "    \n",
    "    for file_type, url in files.items():\n",
    "        temp_filename = download_dir / f\"temp_{split_name}_{file_type}.txt\"\n",
    "        \n",
    "        # Download the file\n",
    "        success = download_file(url, temp_filename)\n",
    "        download_results[split_name][file_type] = success\n",
    "        \n",
    "        if success:\n",
    "            temp_files[file_type] = temp_filename\n",
    "            downloaded_count += 1\n",
    "        else:\n",
    "            all_success = False\n",
    "            break\n",
    "        \n",
    "        # Small delay between downloads\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # If all files downloaded successfully, create CSV\n",
    "    if all_success and len(temp_files) == 3:\n",
    "        try:\n",
    "            print(f\"Creating CSV file for {split_name} split...\")\n",
    "            \n",
    "            # Read all data\n",
    "            with open(temp_files['sentences'], 'r', encoding='utf-8') as f:\n",
    "                sentences = [line.strip() for line in f.readlines()]\n",
    "            \n",
    "            with open(temp_files['sentiments'], 'r', encoding='utf-8') as f:\n",
    "                sentiments = [int(line.strip()) for line in f.readlines()]\n",
    "            \n",
    "            with open(temp_files['topics'], 'r', encoding='utf-8') as f:\n",
    "                topics = [int(line.strip()) for line in f.readlines()]\n",
    "            \n",
    "            # Create DataFrame\n",
    "            data = []\n",
    "            for sentence, sentiment, topic in zip(sentences, sentiments, topics):\n",
    "                data.append({\n",
    "                    'sentence': sentence,\n",
    "                    'sentiment': sentiment,\n",
    "                    'sentiment_label': sentiment_labels[sentiment],\n",
    "                    'topic': topic,\n",
    "                    'topic_label': topic_labels[topic]\n",
    "                })\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Save as CSV\n",
    "            df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "            print(f\"✓ Created {csv_filename.name} with {len(df)} records\")\n",
    "            \n",
    "            # Clean up temporary files\n",
    "            for temp_file in temp_files.values():\n",
    "                temp_file.unlink()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error creating CSV for {split_name}: {e}\")\n",
    "            all_success = False\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"Download completed: {downloaded_count}/{total_files} files processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa145c21",
   "metadata": {},
   "source": [
    "## Verify Downloaded Files\n",
    "\n",
    "Check the downloaded files and display basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e646640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== File Verification ===\n",
      "Download directory: /Users/ducqhle/Documents/AI_Thinking_DoAn/DoAn/vietnamese_feedback_csv\n",
      "\n",
      "✓ train_data.csv:\n",
      "  Size: 1,175,110 bytes\n",
      "  Rows: 11,426\n",
      "  Columns: ['sentence', 'sentiment', 'sentiment_label', 'topic', 'topic_label']\n",
      "\n",
      "✓ validation_data.csv:\n",
      "  Size: 156,743 bytes\n",
      "  Rows: 1,583\n",
      "  Columns: ['sentence', 'sentiment', 'sentiment_label', 'topic', 'topic_label']\n",
      "\n",
      "✓ test_data.csv:\n",
      "  Size: 324,249 bytes\n",
      "  Rows: 3,166\n",
      "  Columns: ['sentence', 'sentiment', 'sentiment_label', 'topic', 'topic_label']\n",
      "\n",
      "Total downloaded size: 1,656,102 bytes (1.58 MB)\n"
     ]
    }
   ],
   "source": [
    "# Verify downloaded CSV files\n",
    "print(\"=== File Verification ===\")\n",
    "print(f\"Download directory: {download_dir.absolute()}\\n\")\n",
    "\n",
    "total_size = 0\n",
    "file_stats = []\n",
    "\n",
    "for split_name in URLS.keys():\n",
    "    csv_filename = download_dir / f\"{split_name}_data.csv\"\n",
    "    \n",
    "    if csv_filename.exists():\n",
    "        file_size = csv_filename.stat().st_size\n",
    "        total_size += file_size\n",
    "        \n",
    "        try:\n",
    "            # Read CSV to get row count and column info\n",
    "            df = pd.read_csv(csv_filename)\n",
    "            row_count = len(df)\n",
    "            columns = list(df.columns)\n",
    "            \n",
    "            print(f\"✓ {csv_filename.name}:\")\n",
    "            print(f\"  Size: {file_size:,} bytes\")\n",
    "            print(f\"  Rows: {row_count:,}\")\n",
    "            print(f\"  Columns: {columns}\")\n",
    "            \n",
    "            file_stats.append({\n",
    "                'split': split_name,\n",
    "                'size': file_size,\n",
    "                'rows': row_count,\n",
    "                'columns': len(columns)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ {csv_filename.name}: Error reading file - {e}\")\n",
    "    else:\n",
    "        print(f\"✗ {split_name}_data.csv: File not found\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"Total downloaded size: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4973c074",
   "metadata": {},
   "source": [
    "## Sample Data Preview\n",
    "\n",
    "Display a few sample records from the training set to verify the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9275eec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample Data Preview (Training Set) ===\n",
      "Sample records:\n",
      "--------------------------------------------------------------------------------\n",
      "Record 1:\n",
      "  Sentence: slide giáo trình đầy đủ .\n",
      "  Sentiment: 2 (positive)\n",
      "  Topic: 1 (training_program)\n",
      "\n",
      "Record 2:\n",
      "  Sentence: nhiệt tình giảng dạy , gần gũi với sinh viên .\n",
      "  Sentiment: 2 (positive)\n",
      "  Topic: 0 (lecturer)\n",
      "\n",
      "Record 3:\n",
      "  Sentence: đi học đầy đủ full điểm chuyên cần .\n",
      "  Sentiment: 0 (negative)\n",
      "  Topic: 1 (training_program)\n",
      "\n",
      "Record 4:\n",
      "  Sentence: chưa áp dụng công nghệ thông tin và các thiết bị hỗ trợ cho việc giảng dạy .\n",
      "  Sentiment: 0 (negative)\n",
      "  Topic: 0 (lecturer)\n",
      "\n",
      "Record 5:\n",
      "  Sentence: thầy giảng bài hay , có nhiều bài tập ví dụ ngay trên lớp .\n",
      "  Sentiment: 2 (positive)\n",
      "  Topic: 0 (lecturer)\n",
      "\n",
      "Dataset shape: 11426 rows, 5 columns\n",
      "Columns: ['sentence', 'sentiment', 'sentiment_label', 'topic', 'topic_label']\n"
     ]
    }
   ],
   "source": [
    "# Preview sample data\n",
    "print(\"=== Sample Data Preview (Training Set) ===\")\n",
    "\n",
    "try:\n",
    "    # Read CSV file\n",
    "    csv_file = download_dir / \"train_data.csv\"\n",
    "    \n",
    "    if csv_file.exists():\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        print(\"Sample records:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Display first 5 records\n",
    "        for i, (_, row) in enumerate(df.head().iterrows(), 1):\n",
    "            print(f\"Record {i}:\")\n",
    "            print(f\"  Sentence: {row['sentence']}\")\n",
    "            print(f\"  Sentiment: {row['sentiment']} ({row['sentiment_label']})\")\n",
    "            print(f\"  Topic: {row['topic']} ({row['topic_label']})\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"Dataset shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "    else:\n",
    "        print(\"Training CSV file not found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading sample data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb06e6e",
   "metadata": {},
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "Display comprehensive statistics about the downloaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "947ff4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Statistics ===\n",
      "Train set: 11,426 sentences\n",
      "  Sentiment distribution: {'positive': np.int64(5643), 'negative': np.int64(5325), 'neutral': np.int64(458)}\n",
      "  Topic distribution: {'lecturer': np.int64(8166), 'training_program': np.int64(2201), 'others': np.int64(562), 'facility': np.int64(497)}\n",
      "\n",
      "Validation set: 1,583 sentences\n",
      "  Sentiment distribution: {'positive': np.int64(805), 'negative': np.int64(705), 'neutral': np.int64(73)}\n",
      "  Topic distribution: {'lecturer': np.int64(1151), 'training_program': np.int64(267), 'others': np.int64(95), 'facility': np.int64(70)}\n",
      "\n",
      "Test set: 3,166 sentences\n",
      "  Sentiment distribution: {'positive': np.int64(1590), 'negative': np.int64(1409), 'neutral': np.int64(167)}\n",
      "  Topic distribution: {'lecturer': np.int64(2290), 'training_program': np.int64(572), 'others': np.int64(159), 'facility': np.int64(145)}\n",
      "\n",
      "Total sentences: 16,175\n",
      "\n",
      "Split distribution:\n",
      "  Train: 70.6%\n",
      "  Validation: 9.8%\n",
      "  Test: 19.6%\n"
     ]
    }
   ],
   "source": [
    "# Dataset statistics\n",
    "print(\"=== Dataset Statistics ===\")\n",
    "\n",
    "total_sentences = 0\n",
    "split_stats = {}\n",
    "\n",
    "for split_name in ['train', 'validation', 'test']:\n",
    "    csv_file = download_dir / f\"{split_name}_data.csv\"\n",
    "    \n",
    "    if csv_file.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            count = len(df)\n",
    "            \n",
    "            split_stats[split_name] = count\n",
    "            total_sentences += count\n",
    "            print(f\"{split_name.capitalize()} set: {count:,} sentences\")\n",
    "            \n",
    "            # Display sentiment distribution\n",
    "            sentiment_dist = df['sentiment_label'].value_counts()\n",
    "            print(f\"  Sentiment distribution: {dict(sentiment_dist)}\")\n",
    "            \n",
    "            # Display topic distribution\n",
    "            topic_dist = df['topic_label'].value_counts()\n",
    "            print(f\"  Topic distribution: {dict(topic_dist)}\")\n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{split_name.capitalize()} set: Error reading file - {e}\")\n",
    "    else:\n",
    "        print(f\"{split_name.capitalize()} set: File not found\")\n",
    "\n",
    "print(f\"Total sentences: {total_sentences:,}\")\n",
    "\n",
    "# Calculate percentages\n",
    "if total_sentences > 0:\n",
    "    print(\"\\nSplit distribution:\")\n",
    "    for split_name, count in split_stats.items():\n",
    "        percentage = (count / total_sentences) * 100\n",
    "        print(f\"  {split_name.capitalize()}: {percentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa3098",
   "metadata": {},
   "source": [
    "## Data Loading Function\n",
    "\n",
    "Create a helper function to load the dataset for further analysis or machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9abfcff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV data loading function created successfully!\n",
      "Sample loaded data structure:\n",
      "  Columns: ['sentence', 'sentiment', 'sentiment_label', 'topic', 'topic_label']\n",
      "  Shape: (3, 5)\n",
      "\n",
      "First record:\n",
      "{'sentence': 'slide giáo trình đầy đủ .', 'sentiment': 2, 'sentiment_label': 'positive', 'topic': 1, 'topic_label': 'training_program'}\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(split='train'):\n",
    "    \"\"\"Load a specific split of the Vietnamese Students' Feedback dataset from CSV.\n",
    "    \n",
    "    Args:\n",
    "        split (str): Dataset split to load ('train', 'validation', or 'test')\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing sentence, sentiment, topic, and labels\n",
    "    \"\"\"\n",
    "    csv_file = download_dir / f\"{split}_data.csv\"\n",
    "    \n",
    "    if not csv_file.exists():\n",
    "        raise FileNotFoundError(f\"CSV file for {split} split not found: {csv_file}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df\n",
    "\n",
    "def load_all_data():\n",
    "    \"\"\"Load all splits and combine them into a single DataFrame with split column.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        try:\n",
    "            df = load_dataset(split)\n",
    "            df['split'] = split\n",
    "            all_data.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {split} split not found, skipping...\")\n",
    "    \n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No dataset files found\")\n",
    "\n",
    "# Test the functions\n",
    "try:\n",
    "    sample_df = load_dataset('train').head(3)\n",
    "    print(\"✓ CSV data loading function created successfully!\")\n",
    "    print(f\"Sample loaded data structure:\")\n",
    "    print(f\"  Columns: {list(sample_df.columns)}\")\n",
    "    print(f\"  Shape: {sample_df.shape}\")\n",
    "    print(\"\\nFirst record:\")\n",
    "    print(sample_df.iloc[0].to_dict())\n",
    "except Exception as e:\n",
    "    print(f\"Error testing data loading function: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3893f2d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Vietnamese Students' Feedback Corpus has been successfully downloaded and saved to CSV files. The dataset is now ready for sentiment analysis and topic classification tasks.\n",
    "\n",
    "### Dataset Information:\n",
    "- **Source**: UIT-VSFC (Vietnamese Students' Feedback Corpus)\n",
    "- **Size**: Over 16,000 annotated sentences\n",
    "- **Tasks**: Sentiment analysis and topic classification\n",
    "- **Splits**: Training, validation, and test sets\n",
    "- **Labels**: \n",
    "  - Sentiment: negative (0), neutral (1), positive (2)\n",
    "  - Topic: lecturer (0), training_program (1), facility (2), others (3)\n",
    "\n",
    "### CSV Files Created:\n",
    "- `vietnamese_feedback_csv/` directory containing CSV files\n",
    "- `train_data.csv` - Training set with all features\n",
    "- `validation_data.csv` - Validation set with all features  \n",
    "- `test_data.csv` - Test set with all features\n",
    "\n",
    "### CSV Structure:\n",
    "Each CSV file contains the following columns:\n",
    "- `sentence`: The feedback text\n",
    "- `sentiment`: Numeric sentiment label (0, 1, 2)\n",
    "- `sentiment_label`: Text sentiment label (negative, neutral, positive)\n",
    "- `topic`: Numeric topic label (0, 1, 2, 3)\n",
    "- `topic_label`: Text topic label (lecturer, training_program, facility, others)\n",
    "\n",
    "### Usage:\n",
    "```python\n",
    "# Load a specific split\n",
    "train_df = load_dataset('train')\n",
    "\n",
    "# Load all data with split information\n",
    "all_df = load_all_data()\n",
    "\n",
    "# Access data using pandas operations\n",
    "positive_feedback = train_df[train_df['sentiment_label'] == 'positive']\n",
    "lecturer_feedback = train_df[train_df['topic_label'] == 'lecturer']\n",
    "```\n",
    "\n",
    "You can now use this data for various NLP tasks including sentiment analysis, topic classification, and other Vietnamese text processing applications using pandas and scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
