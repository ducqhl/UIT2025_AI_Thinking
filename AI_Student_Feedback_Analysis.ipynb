{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c272f0ea",
   "metadata": {},
   "source": [
    "# Ứng dụng AI đánh giá phản hồi học phần từ sinh viên theo cảm xúc và chủ đề\n",
    "\n",
    "Đề tài này nhằm xây dựng một hệ thống sử dụng AI để phân tích phản hồi học phần của sinh viên, xác định cảm xúc (tích cực, tiêu cực, trung lập) và phân loại chủ đề phản hồi (giảng viên, nội dung học phần, cơ sở vật chất, v.v.). Dữ liệu sử dụng là bộ `uitnlp/vietnamese_students_feedback` từ Hugging Face.\n",
    "\n",
    "## Mục tiêu\n",
    "- Phân loại cảm xúc của phản hồi.\n",
    "- Phân loại chủ đề phản hồi.\n",
    "- Đánh giá hiệu quả mô hình và demo dự đoán.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30964935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ducqhle/Library/Python/3.13/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset scripts are no longer supported, but found vietnamese_students_feedback.py",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Tải dữ liệu từ Hugging Face\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muitnlp/vietnamese_students_feedback\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# # Hiển thị một số mẫu dữ liệu\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# print(dataset['train'][0])\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# print(dataset['train'].features)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/datasets/load.py:1392\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1387\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1388\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1389\u001b[39m )\n\u001b[32m   1391\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/datasets/load.py:1132\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1131\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1142\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/datasets/load.py:1031\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m   1026\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1027\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1028\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1030\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/datasets/load.py:989\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    982\u001b[39m     api.hf_hub_download(\n\u001b[32m    983\u001b[39m         repo_id=path,\n\u001b[32m    984\u001b[39m         filename=filename,\n\u001b[32m   (...)\u001b[39m\u001b[32m    987\u001b[39m         proxies=download_config.proxies,\n\u001b[32m    988\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[32m    991\u001b[39m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[32m    992\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision != \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset scripts are no longer supported, but found vietnamese_students_feedback.py"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# Initialize and prepare the dataset\n",
    "vsf = VietnameseStudentsFeedback()\n",
    "vsf.download_and_prepare()\n",
    "\n",
    "# Load the dataset\n",
    "ds = vsf.as_dataset()\n",
    "\n",
    "# Convert to pandas dataframes for easier handling\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame(ds[\"train\"])\n",
    "validation_df = pd.DataFrame(ds[\"validation\"])\n",
    "test_df = pd.DataFrame(ds[\"test\"])\n",
    "\n",
    "print(\"Dataset structure:\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(validation_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[\\n\\r]\", \" \", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Áp dụng tiền xử lý\n",
    "dataset_cleaned = dataset.map(lambda x: {\"text\": preprocess_text(x[\"text\"])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# Tải tokenizer và model PhoBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "model_emotion = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n",
    "\n",
    "# Tokenize dữ liệu\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_emotion = dataset_cleaned.map(tokenize_function, batched=True)\n",
    "\n",
    "# Thiết lập huấn luyện\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_emotion\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs_emotion\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer_emotion = Trainer(\n",
    "    model=model_emotion,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_emotion[\"train\"],\n",
    "    eval_dataset=tokenized_emotion[\"test\"],\n",
    ")\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "trainer_emotion.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo mô hình phân loại chủ đề (giả sử có 5 chủ đề)\n",
    "model_topic = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=5)\n",
    "\n",
    "# Tokenize dữ liệu\n",
    "tokenized_topic = dataset_cleaned.map(tokenize_function, batched=True)\n",
    "\n",
    "# Thiết lập huấn luyện\n",
    "training_args_topic = TrainingArguments(\n",
    "    output_dir=\"./results_topic\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs_topic\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer_topic = Trainer(\n",
    "    model=model_topic,\n",
    "    args=training_args_topic,\n",
    "    train_dataset=tokenized_topic[\"train\"],\n",
    "    eval_dataset=tokenized_topic[\"test\"],\n",
    ")\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "trainer_topic.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đánh giá mô hình cảm xúc\n",
    "emotion_metrics = trainer_emotion.evaluate()\n",
    "print(\"Emotion Classification Metrics:\", emotion_metrics)\n",
    "\n",
    "# Đánh giá mô hình chủ đề\n",
    "topic_metrics = trainer_topic.evaluate()\n",
    "print(\"Topic Classification Metrics:\", topic_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c11581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model_emotion(**inputs)\n",
    "    predicted = torch.argmax(outputs.logits, dim=1)\n",
    "    return predicted.item()\n",
    "\n",
    "def predict_topic(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model_topic(**inputs)\n",
    "    predicted = torch.argmax(outputs.logits, dim=1)\n",
    "    return predicted.item()\n",
    "\n",
    "# Ví dụ dự đoán\n",
    "sample_text = \"Giảng viên giảng bài rất dễ hiểu và nhiệt tình.\"\n",
    "print(\"Emotion:\", predict_emotion(sample_text))\n",
    "print(\"Topic:\", predict_topic(sample_text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
